{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Student Information**\n",
    "Name:賴奕叡\n",
    "\n",
    "Student ID:110048224\n",
    "\n",
    "GitHub ID:hibernater030\n",
    "\n",
    "Kaggle name:ss030113\n",
    "\n",
    "Kaggle private scoreboard snapshot: \n",
    "\n",
    "![pic_ranking.png](./pics/pic_ranking.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Instructions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lab we have divided the assignments into **three phases/parts**. The `first two phases` refer to the `exercises inside the Master notebooks` of the [DM2025-Lab2-Exercise Repo](https://github.com/difersalest/DM2025-Lab2-Exercise.git). The `third phase` refers to an `internal Kaggle competition` that we are gonna run among all the Data Mining students. Together they add up to `100 points` of your grade. There are also some `bonus points` to be gained if you complete `extra exercises` in the lab **(bonus 15 pts)** and in the `Kaggle Competition report` **(bonus 5 pts)**.\n",
    "\n",
    "**Environment recommendations to solve lab 2:**\n",
    "- **Phase 1 exercises:** Need GPU for training the models explained in that part, if you don't have a GPU in your laptop it is recommended to run in Colab or Kaggle for a faster experience, although with CPU they can still be solved but with a slower execution.\n",
    "- **Phase 2 exercises:** We use Gemini's API so everything can be run with only CPU without a problem.\n",
    "- **Phase 3 exercises:** For the competition you will probably need GPU to train your models, so it is recommended to use Colab or Kaggle if you don't have a laptop with a dedicated GPU.\n",
    "- **Optional Ollama Notebook (not graded):** You need GPU, at least 4GB of VRAM with 16 GB of RAM to run the local open-source LLM models. \n",
    "\n",
    "## **Phase 1 (30 pts):**\n",
    "\n",
    "1. __Main Exercises (25 pts):__ Do the **take home exercises** from Sections: `1. Data Preparation` to `9. High-dimension Visualization: t-SNE and UMAP`, in the [DM2025-Lab2-Master-Phase_1 Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_1.ipynb). Total: `8 exercises`. Commit your code and submit the repository link to NTU Cool **`BEFORE the deadline (Nov. 3th, 11:59 pm, Monday)`**\n",
    "\n",
    "2. **Code Comments (5 pts):** **Tidy up the code in your notebook**. \n",
    "\n",
    "## **Phase 2 (30 pts):**\n",
    "\n",
    "1. **Main Exercises (25 pts):** Do the remaining **take home exercises** from Section: `2. Large Language Models (LLMs)` in the [DM2025-Lab2-Master-Phase_2_Main Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_2_Main.ipynb). Total: `5 exercises required from sections 2.1, 2.2, 2.4 and 2.6`. Commit your code and submit the repository link to NTU Cool **`BEFORE the deadline (Nov. 24th, 11:59 pm, Monday)`**\n",
    "\n",
    "2. **Code Comments (5 pts):** **Tidy up the code in your notebook**. \n",
    "\n",
    "3. **`Bonus (15 pts):`** Complete the bonus exercises in the [DM2025-Lab2-Master-Phase_2_Bonus Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_2_Bonus.ipynb) and [DM2025-Lab2-Master-Phase_2_Main Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_2_Main.ipynb) `where 2 exercises are counted as bonus from sections 2.3 and 2.5 in the main notebook`. Total: `7 exercises`. Commit your code and submit the repository link to NTU Cool **`BEFORE the deadline (Nov. 24th, 11:59 pm, Monday)`**\n",
    "\n",
    "## **Phase 3 (40 pts):**\n",
    "\n",
    "1. **Kaggle Competition Participation (30 pts):** Participate in the in-class **Kaggle Competition** regarding Emotion Recognition on Twitter by clicking in this link: **[Data Mining Class Kaggle Competition](https://www.kaggle.com/t/3a2df4c6d6b4417e8bf718ed648d7554)**. The scoring will be given according to your place in the Private Leaderboard ranking: \n",
    "    - **Bottom 40%**: Get 20 pts of the 30 pts in this competition participation part.\n",
    "\n",
    "    - **Top 41% - 100%**: Get (0.6N + 1 - x) / (0.6N) * 10 + 20 points, where N is the total number of participants, and x is your rank. (ie. If there are 100 participants and you rank 3rd your score will be (0.6 * 100 + 1 - 3) / (0.6 * 100) * 10 + 20 = 29.67% out of 30%.)   \n",
    "    Submit your last submission **`BEFORE the deadline (Nov. 24th, 11:59 pm, Monday)`**. Make sure to take a screenshot of your position at the end of the competition and store it as `pic_ranking.png` under the `pics` folder of this repository and rerun the cell **Student Information**.\n",
    "\n",
    "2. **Competition Report (10 pts)** A report section to be filled in inside this notebook in Markdown Format, we already provided you with the template below. You need to describe your work developing the model for the competition. The report should include a section describing briefly the following elements: \n",
    "* Your preprocessing steps.\n",
    "* The feature engineering steps.\n",
    "* Explanation of your model.\n",
    "\n",
    "* **`Bonus (5 pts):`**\n",
    "    * You will have to describe more detail in the previous steps.\n",
    "    * Mention different things you tried.\n",
    "    * Mention insights you gained. \n",
    "\n",
    "[Markdown Guide - Basic Syntax](https://www.markdownguide.org/basic-syntax/)\n",
    "\n",
    "**`Things to note for Phase 3:`**\n",
    "\n",
    "* **The code used for the competition should be in this Jupyter Notebook File** `DM2025-Lab2-Homework.ipynb`.\n",
    "\n",
    "* **Push the code used for the competition to your repository**.\n",
    "\n",
    "* **The code should have a clear separation for the same sections of the report, preprocessing, feature engineering and model explanation. Briefly comment your code for easier understanding, we provide a template at the end of this notebook.**\n",
    "\n",
    "* Showing the kaggle screenshot of the ranking plus the code in this notebook will ensure the validity of your participation and the report to obtain the corresponding points.\n",
    "\n",
    "After the competition ends you will have two days more to submit the `DM2025-Lab2-Homework.ipynb` with your report in markdown format and your code. Do everything **`BEFORE the deadline (Nov. 26th, 11:59 pm, Wednesday) to obtain 100% of the available points.`**\n",
    "\n",
    "Upload your files to your repository then submit the link to it on the corresponding NTU Cool assignment.\n",
    "\n",
    "## **Deadlines:**\n",
    "\n",
    "![lab2_deadlines](./pics/lab2_deadlines.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Next you will find the template report with some simple markdown syntax explanations, use it to structure your content.\n",
    "\n",
    "You can delete the syntax suggestions after you use them.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# **Project Report**\n",
    "\n",
    "## 1. Model Development (10 pts Required)\n",
    "\n",
    "\n",
    "---\n",
    "**本次實作於google colab上進行，以確保有足夠的GPU資源訓練模型。**\n",
    "\n",
    "為確保模型能使用乾淨的資料集，我設計的前處理流程如程式碼所示，主要包含以下幾個關鍵步驟：\n",
    "* **巢狀JSON資料解析**：final_posts.json有複雜的巢狀結構(root -> _source -> post)。一般的 pd.read_json無法直接讀取，所以我撰寫能解析的程式碼，提取post_id與text欄位，並解決了部分檔案欄位名稱不一致（如id vs tweet_id）的問題，確保資料能正確合併。\n",
    "\n",
    "![Example Markdown Syntax to Add Image](./pics/001.png)\n",
    "* **資料合併**：將解析後的文字資料與data_identification.csv(ID 對照表)及emotion.csv(情緒標籤)進行合併。合併後再根據split欄位將資料切分為訓練集與測試集。\n",
    "\n",
    "![Example Markdown Syntax to Add Image](./pics/002.png)\n",
    "* **文字清洗**\n",
    "    * **移除雜訊**：移除URL與User Handles(@user)，這些資訊通常不含情緒特徵且可能造成模型偏差。\n",
    "    * **Hashtag處理**：移除了#但保留後面文字，因Hashtag往往包含關鍵的情緒詞彙。\n",
    "    * **格式整理**：移除多餘的空白字元與非列印字元。\n",
    "![Example Markdown Syntax to Add Image](./pics/003.png)    \n",
    "* **標籤編碼**：用LabelEncoder將八種文字情緒類別轉換為數值ID(0-7)，以符合CrossEntropyLoss的輸入要求。\n",
    "\n",
    "![Example Markdown Syntax to Add Image](./pics/004.png) \n",
    "\n",
    "完成上述處理就能切分Train/Validation Set\n",
    "\n",
    "### 1.2 Feature Engineering Steps\n",
    "\n",
    "特徵工程的部分，我沒使用傳統的手動特徵提取方法(如TF-IDF或Bow)，而是嘗試挑戰用Transformer架構的自動特徵提取能力。以下為詳細解說：\n",
    "* **Tokenizer初始化與選擇**：這邊我用RoBERTa的AutoTokenizer，這個Tokenizer基於BPE演算法。與傳統以單字為單位的分詞不同，BPE能將陌生單字拆解為有意義的子詞。能有效解決Out-Of-Vocabulary的問題。\n",
    "\n",
    "![Example Markdown Syntax to Add Image](./pics/005.png) \n",
    "* **輸入格式轉換**：用tokenizer.encode_plus函數將每則推文轉換為模型所需的兩個張量(如下)\n",
    "    * **input_ids**：把文字映射到RoBERTa詞彙表中的數值索引。\n",
    "    * **attention_mask**：產生二元遮罩，標記哪些是真實的文字token (1)，哪些是補零的padding token (0)，確保模型在計算Attention時只關注有效內容。\n",
    "\n",
    "![Example Markdown Syntax to Add Image](./pics/006.png) \n",
    "* **序列長度設定**\n",
    "    * **MAX_LEN = 128**：我設最大序列長度為128，因為考慮到多數內文的長度短，這個長度應該足以涵蓋絕大部分的語意資訊，也能避免過度佔用GPU。\n",
    "    * **Padding & Truncation**：針對短於128的進行補零，長於128的進行截斷，確保輸入Batch的維度一致。\n",
    "\n",
    "![Example Markdown Syntax to Add Image](./pics/007.png) \n",
    "* **資料管線建構**\n",
    "    * **Dataset類別**：繼承PyTorch的Dataset類別，封裝資料讀取與Tokenization的邏輯。\n",
    "    * **DataLoader**：設定BATCH_SIZE = 32，並啟用shuffle=True，建立Mini-batch資料載入器，有助於模型訓練時的梯度下降收斂更穩。\n",
    "\n",
    "![Example Markdown Syntax to Add Image](./pics/008.png) \n",
    "\n",
    "### 1.3 Explanation of Your Model\n",
    "\n",
    "這邊採用**Transfer Learning**的策略。不從頭開始訓練神經網路，而是載入已經在大量文本上訓練過的權重，並針對本次的情緒分類任務進行微調。\n",
    "\n",
    "* **核心模型架構說明**：\n",
    "    * **模型選擇**：`cardiffnlp/twitter-roberta-base-emotion`，這是基於RoBERTa的model。與標準BERT不同，這模型在大量的推文上進行過預訓練。因此對推特的非正式文法、網路用語及Emoji有很大的理解優勢，我認為非常適合這個競賽的資料集特性。\n",
    "* **訓練策略與優化**：\n",
    "    * **優化器**：使用AdamW，這是目前訓練Transformer模型最標準的選擇，能有效處理稀疏梯度、透過權重衰減防止過擬合。\n",
    "    * **學習率**：設定為`2e-5`，算很小的學習率，目的是在微調過程中保留預訓練模型既有的語言知識，避免因更新過大而破壞原有的權重結構。\n",
    "    * **學習率排程**：使用`get_linear_schedule_with_warmup`。\n",
    "        * **Warmup**：在訓練初期線性增加學習率，幫助模型穩定啟動。\n",
    "        * **Linear Decay**：隨後線性降低學習率，幫助模型在訓練後期更精細地收斂到最佳解。\n",
    "\n",
    "![Example Markdown Syntax to Add Image](./pics/009.png)\n",
    "* **訓練穩定性機制**：\n",
    "    * **梯度裁減**：在程式碼中實作`nn.utils.clip_grad_norm_`。能防止在反向傳播過程中發生梯度爆炸、確保訓練過程的穩定性。\n",
    "\n",
    "* **損失函數**：\n",
    "    * 使用`CrossEntropyLoss`，多類別分類任務的標準損失函數，用來計算模型預測機率分佈與真實標籤之間的差異。\n",
    "\n",
    "![Example Markdown Syntax to Add Image](./pics/010.png)\n",
    "---\n",
    "\n",
    "## 2. Bonus Section (5 pts Optional)\n",
    "\n",
    "**Add more detail in previous sections**\n",
    "\n",
    "### 2.1 Mention Different Things You Tried\n",
    "\n",
    "為探討不同技術/模型對情緒識別的影響，我依序嘗試過「傳統機器學習」與「深度學習」方法，並針對模型弱點進行了優化實驗。\n",
    "\n",
    "我進行了三組不同的變化來比較不同模型架構與優化策略的效果，也附上三種方法的結果截圖：\n",
    "\n",
    "**一、傳統機器學習基準(TF-IDF + Naive Bayes)**\n",
    "* **方法**：使用TF-IDF提取關鍵字特徵，搭配Naive Bayes分類器\n",
    "* **結果**：驗證集準確率(Val Acc)為0.58\n",
    "* **結果觀察**：\n",
    "    * 模型只能識別關鍵字明顯的'Joy'(Recall:0.86)\n",
    "    * 對於語意複雜或樣本少的類別幾乎無效，例如 **'Disgust'的Recall只有 0.01**，'Sadness'也只有0.11。這顯示傳統Bag-of-Words方法無法捕捉推文中隱晦的情緒語意\n",
    "\n",
    "![Example Markdown Syntax to Add Image](./pics/011.png)\n",
    "\n",
    "**二、深度學習(RoBERTa，為Kaggle最終採用的方案)**\n",
    "* **方法**：使用`twitter-roberta-base-emotion`進行微調(Fine-tuning)，訓練3 Epochs\n",
    "* **結果**：驗證集準確率大幅提升至**0.69**\n",
    "* **結果觀察**：相較於TF-IDF，準確率提升了11%。能證明Transformer架構能理解上下文與Emoji的意涵，而不僅是看關鍵字。但模型仍存在傾向預測多數類別的問題\n",
    "\n",
    "![Example Markdown Syntax to Add Image](./pics/013.png)\n",
    "\n",
    "**三、解決資料不平衡(Experiment: Weighted Loss)**\n",
    "* **方法**：為了改善方法二中少數類別識別率低的問題，我嘗試引入**類別權重 (Class Weights)**至損失函數，並將訓練輪數增加至 5 Epochs\n",
    "* **結果**：驗證集準確率下降至**0.64**\n",
    "* **結果觀察**：\n",
    "    * **優點**：有成功拉起被忽略的情緒，如'Disgust'的Recall從方法二的低點顯著提升 (0.15 $\\rightarrow$ 0.32)\n",
    "    * **缺點**：為了照顧少數類別，模型犧牲對多數類別'Joy'的預測自信度，最終導致整體準確率下降\n",
    "\n",
    "![Example Markdown Syntax to Add Image](./pics/012.png)\n",
    "\n",
    "**四、傳統機器學習(TF-IDF + Logistic Regression)**\n",
    "* **結果**：驗證集準確率提升至 **0.63**\n",
    "* **結果觀察**：\n",
    "    * 相較於方法一的Naive Bayes，LR的效能有所提升\n",
    "    * 極端的類別偏好：然而LR對多數類別 **'Joy'** 的偏好更為強烈 (**Recall高達0.89**)，但對少數類別依然沒什麼用(如 **'Disgust'Recall僅0.03**)。能推測傳統線性模型在極度不平衡的資料集中，容易透過「過度預測多數類別」來衝高整體準確率，但並未真正學會區分少數情緒\n",
    "\n",
    "![Example Markdown Syntax to Add Image](./pics/015.png)\n",
    "\n",
    "### 2.2 Mention Insights You Gained\n",
    "\n",
    "**1.關鍵字&語意理解**：\n",
    "* 比較方法一(0.58)與方法二(0.69)，顯著的效能差距能證實在推特這種非正式、短文本的情況中，**預訓練語言模型 (PLMs)**是必要的，傳統的TF-IDF容易被關鍵字，如`not happy`裡的`happy`誤導，而RoBERTa能正確理解否定語氣。\n",
    "\n",
    "**2.準確率不一定等於好**：\n",
    "* 方法三的實作能看見在極度不平衡的資料集中，單看Accuracy是有疑慮的。雖然方法三分數較低(0.64)，但它對'Disgust'和'Fear'的識別能力其實應該比方法二更好。我想未來如果要改進，應考慮使用F1-Macro作為優化目標，而非只單純追求Accuracy。\n",
    "\n",
    "**3.混淆矩陣分析**：\n",
    "* 在六種情緒中 **'Surprise'** 的表現屬於中等 (F1: 0.55)。我推測這是因為Surprise在人類的語意上具有雙重性，可能是正面的(Joy)也可能是負面的(Fear)。缺乏更長的上下文的話，模型會難憑一句推文精確判斷其極性，導致分類難度高於單純的Joy or Anger。\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`From here on starts the code section for the competition.`**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Competition Code**\n",
    "\n",
    "## 1. Preprocessing Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 本次實作是在Google colab上執行\n",
    "# 資料集從Google drive匯入、結果匯出到Google drive\n",
    "\n",
    "# 安裝必要套件\n",
    "!pip install transformers accelerate -q\n",
    "!pip install torch scikit-learn pandas numpy -q\n",
    "\n",
    "# 載入套件\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "from google.colab import drive\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# 掛載Google drive (教授or助教若有需要，可於此修改匯入資料集的路徑)\n",
    "if not os.path.exists('/content/drive'):\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "DATA_DIR = '/content/drive/MyDrive/phase3_dataset'\n",
    "OUTPUT_DIR = '/content/drive/MyDrive/phase3_submission'\n",
    "\n",
    "print(f\"讀取資料夾： {DATA_DIR}\")\n",
    "\n",
    "try:\n",
    "    # 讀取ID對照表\n",
    "    df_id = pd.read_csv(f'{DATA_DIR}/data_identification.csv')\n",
    "    if 'id' in df_id.columns: df_id.rename(columns={'id': 'tweet_id'}, inplace=True)\n",
    "    \n",
    "    # 讀取Emotion Labels\n",
    "    df_emotion = pd.read_csv(f'{DATA_DIR}/emotion.csv')\n",
    "    if 'id' in df_emotion.columns: df_emotion.rename(columns={'id': 'tweet_id'}, inplace=True)\n",
    "    \n",
    "    # 讀取、解析JSON Posts\n",
    "    with open(f'{DATA_DIR}/final_posts.json', 'r') as f:\n",
    "        posts_data = json.load(f)\n",
    "    \n",
    "    parsed_posts = []\n",
    "    for item in posts_data:\n",
    "        try:\n",
    "            # 解析巢狀結構\n",
    "            post_info = item['root']['_source']['post']\n",
    "            parsed_posts.append({\n",
    "                'tweet_id': post_info['post_id'],\n",
    "                'text': post_info['text']\n",
    "            })\n",
    "        except KeyError:\n",
    "            continue\n",
    "            \n",
    "    df_posts = pd.DataFrame(parsed_posts)\n",
    "    print(f\"完成解析 {len(df_posts)} 筆貼文資料\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"\\n讀取錯誤: {e}\")\n",
    "    raise e\n",
    "\n",
    "# 資料Merging\n",
    "df_id['tweet_id'] = df_id['tweet_id'].astype(str)\n",
    "df_posts['tweet_id'] = df_posts['tweet_id'].astype(str)\n",
    "df_emotion['tweet_id'] = df_emotion['tweet_id'].astype(str)\n",
    "\n",
    "# 合併ID、文字\n",
    "df_full = pd.merge(df_id, df_posts, on='tweet_id', how='left')\n",
    "print(f\"資料集欄位: {df_full.columns.tolist()}\")\n",
    "\n",
    "if 'split' in df_full.columns:\n",
    "    # 使用 'split' 欄位來區分 train 和 test\n",
    "    df_train = df_full[df_full['split'] == 'train'].copy()\n",
    "    df_test = df_full[df_full['split'] == 'test'].copy()\n",
    "else:\n",
    "    # 欄位名稱防呆機制\n",
    "    print(\"找不到'split'，改使用 'identification'...\")\n",
    "    df_train = df_full[df_full['identification'] == 'train'].copy()\n",
    "    df_test = df_full[df_full['identification'] == 'test'].copy()\n",
    "\n",
    "# 合併情緒標籤\n",
    "df_train = pd.merge(df_train, df_emotion, on='tweet_id', how='left')\n",
    "\n",
    "print(f\"訓練集數量: {len(df_train)}\")\n",
    "print(f\"測試集數量: {len(df_test)}\")\n",
    "\n",
    "if df_train['text'].isnull().sum() > 0:\n",
    "    print(f\"有 {df_train['text'].isnull().sum()} 筆訓練資料遺失文字內容\")\n",
    "\n",
    "# 文字清洗\n",
    "def preprocess_tweet(text):\n",
    "    if not isinstance(text, str): return \"\"\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "df_train['clean_text'] = df_train['text'].apply(preprocess_tweet)\n",
    "df_test['clean_text'] = df_test['text'].apply(preprocess_tweet)\n",
    "\n",
    "# 標籤編碼\n",
    "le = LabelEncoder()\n",
    "df_train = df_train.dropna(subset=['emotion'])\n",
    "df_train['label_id'] = le.fit_transform(df_train['emotion'])\n",
    "NUM_LABELS = len(le.classes_)\n",
    "\n",
    "# 切分Train/Validation Set\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    df_train['clean_text'], \n",
    "    df_train['label_id'], \n",
    "    test_size=0.2, \n",
    "    random_state=42,\n",
    "    stratify=df_train['label_id']\n",
    ")\n",
    "\n",
    "# 列出情緒類別\n",
    "print(f\"情緒類別: {le.classes_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 載入套件\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# 設定與準備Tokenizer\n",
    "# 我使用RoBERTa\n",
    "MODEL_NAME = \"cardiffnlp/twitter-roberta-base-emotion\"\n",
    "\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "except Exception as e:\n",
    "    print(f\"下載失敗: {e}\")\n",
    "    raise e\n",
    "\n",
    "# 設定參數\n",
    "MAX_LEN = 128\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# 定義Dataset類別\n",
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, texts, labels=None, tokenizer=None, max_len=128):\n",
    "        self.texts = texts.reset_index(drop=True)\n",
    "        self.labels = labels.reset_index(drop=True) if labels is not None else None\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        text = str(self.texts[item])\n",
    "        \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        \n",
    "        output = {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten()\n",
    "        }\n",
    "        \n",
    "        if self.labels is not None:\n",
    "            output['labels'] = torch.tensor(self.labels[item], dtype=torch.long)\n",
    "            \n",
    "        return output\n",
    "\n",
    "# 建立Dataset\n",
    "train_dataset = EmotionDataset(X_train, y_train, tokenizer, MAX_LEN)\n",
    "val_dataset = EmotionDataset(X_val, y_val, tokenizer, MAX_LEN)\n",
    "test_dataset = EmotionDataset(df_test['clean_text'], None, tokenizer, MAX_LEN)\n",
    "\n",
    "# 建立DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "print(f\"Train Batch數: {len(train_loader)}\")\n",
    "print(f\"Val Batch數: {len(val_loader)}\")\n",
    "print(f\"Test Batch數: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Implementation Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import os\n",
    "\n",
    "# 載入模型與設定環境\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME, \n",
    "    num_labels=NUM_LABELS,\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "# 設定訓練參數\n",
    "EPOCHS = 3\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "total_steps = len(train_loader) * EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, \n",
    "    num_warmup_steps=0, \n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "# 定義訓練與驗證函式\n",
    "def train_epoch(model, data_loader, epoch_idx):\n",
    "    model = model.train()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    n_examples = 0\n",
    "    \n",
    "    print(f\"正在訓練Epoch {epoch_idx+1}/{EPOCHS}\")\n",
    "    \n",
    "    for i, d in enumerate(data_loader):\n",
    "        input_ids = d[\"input_ids\"].to(device)\n",
    "        attention_mask = d[\"attention_mask\"].to(device)\n",
    "        targets = d[\"labels\"].to(device)\n",
    "        \n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        \n",
    "        _, preds = torch.max(outputs.logits, dim=1)\n",
    "        loss = loss_fn(outputs.logits, targets)\n",
    "        \n",
    "        correct_predictions += torch.sum(preds == targets)\n",
    "        losses.append(loss.item())\n",
    "        n_examples += targets.size(0)\n",
    "        \n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 每100個batch印一次進度，才不會一直呆呆的等\n",
    "        if i % 100 == 0 and i > 0:\n",
    "            print(f\"  Batch {i}/{len(data_loader)} - Loss: {loss.item():.4f}\")\n",
    "            \n",
    "    return correct_predictions.double() / n_examples, np.mean(losses)\n",
    "\n",
    "def eval_model(model, data_loader):\n",
    "    model = model.eval()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    n_examples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for d in data_loader:\n",
    "            input_ids = d[\"input_ids\"].to(device)\n",
    "            attention_mask = d[\"attention_mask\"].to(device)\n",
    "            targets = d[\"labels\"].to(device)\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "            _, preds = torch.max(outputs.logits, dim=1)\n",
    "            loss = loss_fn(outputs.logits, targets)\n",
    "            \n",
    "            correct_predictions += torch.sum(preds == targets)\n",
    "            losses.append(loss.item())\n",
    "            n_examples += targets.size(0)\n",
    "            \n",
    "    return correct_predictions.double() / n_examples, np.mean(losses)\n",
    "\n",
    "# 開始訓練迴圈\n",
    "history = {'train_acc': [], 'train_loss': [], 'val_acc': [], 'val_loss': []}\n",
    "\n",
    "print(\"開始訓練模型\")\n",
    "for epoch in range(EPOCHS):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_acc, train_loss = train_epoch(model, train_loader, epoch)\n",
    "    val_acc, val_loss = eval_model(model, val_loader)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    # 印個結果方便觀察\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS} 完成 | 耗時: {end_time - start_time:.0f}秒')\n",
    "    print(f'Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}')\n",
    "    print(f'Val   Loss: {val_loss:.4f} | Val   Acc: {val_acc:.4f}')\n",
    "    print('-' * 30)\n",
    "    \n",
    "    history['train_acc'].append(train_acc.item())\n",
    "    history['val_acc'].append(val_acc.item())\n",
    "\n",
    "# 預測測試集並存檔\n",
    "print(\"\\n訓練完成，預測測試集\")\n",
    "model.eval()\n",
    "test_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for d in test_loader:\n",
    "        input_ids = d[\"input_ids\"].to(device)\n",
    "        attention_mask = d[\"attention_mask\"].to(device)\n",
    "        \n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        _, preds = torch.max(outputs.logits, dim=1)\n",
    "        test_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "# 將數字標籤轉回文字\n",
    "final_labels = le.inverse_transform(test_preds)\n",
    "\n",
    "# 建立Submission DataFrame\n",
    "submission = pd.DataFrame()\n",
    "submission['id'] = df_test['tweet_id']\n",
    "submission['emotion'] = final_labels\n",
    "\n",
    "# 檢查並建立輸出資料夾(我把結果輸出到自己的Google drive)\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "    print(f\"建立資料夾: {OUTPUT_DIR}\")\n",
    "\n",
    "save_path = os.path.join(OUTPUT_DIR, 'submission.csv') # 檔名在這裡\n",
    "submission.to_csv(save_path, index=False)\n",
    "\n",
    "print(\"=\" * 40)\n",
    "print(f\"檔案已儲存至: {save_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DM2025-Lab2-Exercise (3.11.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
